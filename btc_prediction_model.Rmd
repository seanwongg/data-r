---
title: "Bitcoin Momentum Feature Based Forecasting"
description: |
  This document describes the workflow of Machine Learning applied into a Bitcoin trading strategy
author:
  - name: Vitus Sean Wong
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
    toc_depth: 4
---

In this report, we will highlight the basics of implementing a **Machine Learning workflow** to produce a trading strategy for Bitcoin (BTC) based on historical data. `tidymodels` has simplified machine learning, by creating a clear, logical and systematic approach. `tidymodels` makes the process one that builds upon the previous step, which will be further explained in detail in this report.  

This process entails:  
1. [Data Gathering/Combining Features to Include](#Section2)  
2. [Model Exploration, Training and Evaluation](#Section4)   
3. [Feature Selection](#Section5)   
4. [Ensembling Models](#Section6)  

# 1. Load Packages

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE,
  comment = "#",
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  cache = FALSE,
  fig.align = "center",
  class.source = 'white',
  tidy.opts=list(width.cutoff=60),
  tidy=TRUE
)
rm(list=ls())
gc()
```

```{r}
library(tidyverse)
library(stringr)
library(gtrendsR)
library(Quandl)
library(quantmod)
library(RcppRoll)
library(lubridate)
library(tidyquant)
# devtools::install_github("curso-r/treesnip")
library(treesnip)
library(tidymodels)
library(tsfeatures)
library(slider)
library(timetk)
library(data.table)
library(xts)
library(TTR)
library(PerformanceAnalytics)
library(catboost)
```

<div id = "Section2"></div>
# 2. Scrape Bitcoin Data

## 2.1 Quandl Functions

The `quandl_tidy` function is a wrapper around the Quandl function that returns a cleaner tibble. Namely, we only want to get the respective `code`, its `date` and `value`.

```{r}
Quandl.api_key("API_KEY_HERE") 

quandl_tidy <- function(code, name) { 
  df <- Quandl(code) %>% 
    mutate(code = code, name = name) %>% 
    rename(date = Date, value = Value) %>% 
    arrange(date) %>% 
    as_tibble()
  return(df)
}
```

## 2.2 Bitcoin Exchange Rate Data

```{r}
bitcoin_price <- Quandl("BCHARTS/BITSTAMPUSD") %>%
  arrange(Date) %>%
  as_tibble()

colnames(bitcoin_price) <- c("date", 
                             "open", 
                             "high", 
                             "low", 
                             "close", 
                             "volume_btc", 
                             "volume_currency", 
                             "weighted_price")
```

<div id ="Section23"></div>
## 2.3 Bitcoin Indicators

This section allows us to pull relevant BTC information with `quandl_tidy`. Relevant features such as BTC Market Capitalisation, Hash Rate and BTC Days Destroyed that measures the transaction volume of BTC.

```{r}
code_list <- list(c("BCHAIN/TOTBC", "Total Bitcoins"), 
                  c("BCHAIN/MKTCP", "Bitcoin Market Capitalization"), 
                  c("BCHAIN/NADDU", "Bitcoin Number of Unique Addresses Used"), 
                  c("BCHAIN/ETRAV", "Bitcoin Estimated Transaction Volume BTC"), 
                  c("BCHAIN/ETRVU", "Bitcoin Estimated Transaction Volume USD"), 
                  c("BCHAIN/TRVOU", "Bitcoin USD Exchange Trade Volume"), 
                  c("BCHAIN/NTRAN", "Bitcoin Number of Transactions"), 
                  c("BCHAIN/NTRAT", "Bitcoin Total Number of Transactions"), 
                  c("BCHAIN/NTREP", "Bitcoin Number of Transactions Excluding Popular Addresses"), 
                  c("BCHAIN/NTRBL", "Bitcoin Number of Tansaction per Block"), 
                  c("BCHAIN/ATRCT", "Bitcoin Median Transaction Confirmation Time"), 
                  c("BCHAIN/TRFEE", "Bitcoin Total Transaction Fees"), 
                  c("BCHAIN/TRFUS", "Bitcoin Total Transaction Fees USD"), 
                  c("BCHAIN/CPTRA", "Bitcoin Cost Per Transaction"), 
                  c("BCHAIN/CPTRV", "Bitcoin Cost % of Transaction Volume"), 
                  c("BCHAIN/BLCHS", "Bitcoin api.blockchain Size"), 
                  c("BCHAIN/AVBLS", "Bitcoin Average Block Size"), 
                  c("BCHAIN/TOUTV", "Bitcoin Total Output Volume"), 
                  c("BCHAIN/HRATE", "Bitcoin Hash Rate"), 
                  c("BCHAIN/BCDDE", "Bitcoin Days Destroyed"), 
                  c("BCHAIN/BCDDW", "Bitcoin Days Destroyed Minimum Age 1 Week"), 
                  c("BCHAIN/BCDDM", "Bitcoin Days Destroyed Minimum Age 1 Month"), 
                  c("BCHAIN/BCDDY", "Bitcoin Days Destroyed Minimum Age 1 Year") ,
                  c("BCHAIN/BCDDC", "Bitcoin Days Destroyed Cumulative"))

bitcoin_data <- tibble()

for (i in seq_along(code_list)) { 
  
  print(str_c("Downloading data for ", code_list[[i]][1], "."))
  
  bitcoin_data <- bind_rows(bitcoin_data, 
                            quandl_tidy(code_list[[i]][1], code_list[[i]][2]))
  
}

bitcoin_data <- bitcoin_data %>%
  select(-name) %>%
  spread(code, value)

colnames(bitcoin_data) <- make.names(colnames(bitcoin_data))
```

```{r, echo = FALSE}
rm(code_list, i, quandl_tidy)
```

<div id = "Section3"></div>
# 3. Feature Engineering  

Using the `tidymodel` package, the framework for machine learning has been made easier. Removing or adding features only requires __2 steps__:  

* Loading data of the selected feature  
* Joining to the rest of the features by `date`  

Thus, `Section 3` is non-exhaustive, and features can be freely added and removed. In choosing which feature to include, we mostly included technical indicators and metrics we believe influence BTC's prices.  

**Online Sentiments** would be an intetgral feature in the future, however since the majority of the population are still largely resistant to the promise of cryptocurrency, we have decided to omit these sentiments. **Google Trends** searches were also omitted, as we believe the mass public's interest on BTC will still be slow in the near future, hence online search trends by the masses will not have much fluctuations, hence less impact on BTC prices, furthermore, **Google Trends** have an upper limit of 100 which acts as a upper barrier and would not aid us in modeling when public interest of cryptocurrency is high.

## 3.1 Cleaning Bitcoin Prices

Data errors are cleaned by using `na.locf` (last observations carried forward).

```{r}
bitcoin_price[bitcoin_price == 0] <- NA
bitcoin_price <- bitcoin_price %>%
  map_df(na.locf)
```

## 3.2 Define Target  
Our target will be using a `binary classifcation`, whereby if the future return is positive, it will be indicated with `1`. If the future return is negative, it will be indicated with a `0`. `tq_mutate()` introduces an easy method to calculate periodic returns. 

```{r}
transaction_cost <- 0.003 #assumption

bitcoin_model <- bitcoin_price %>%
  tq_mutate(select = close,
            mutate_fun = periodReturn,
            period = 'daily',
            type = 'arithmetic',
            col_rename = 'future_return') %>%
  mutate(future_return_sign = as.factor(
    ifelse(
      lag(future_return)<0,
      ifelse(future_return > transaction_cost, 1, 0),
      ifelse(future_return > 0, 1, 0))
    ),
    close = lag(close, 1),
    date = date - days(1)) %>%
  select(date, close, future_return, future_return_sign) 

bitcoin_model <- bitcoin_model[-1, ]
```


## 3.3 VIX

**VIX** is a ticker symbol that represents the CBOE Volatility Index which measures the stock market's expectation of volatility based on S&P 500 options. Given that BTC is highly volatile, and given its __growing perception__ as digital gold, we want to investigate if there is any relationship between the fear index of traditional markets to BTC.  

```{r}
get_yahoo <- function(ticker) {
  
  df <- getSymbols(ticker, src = 'yahoo', auto.assign = FALSE, from = '1900-01-01')
  
  df <- df %>%
    as_tibble() %>%
    mutate(date = index(df))
  
  colnames(df) <- c("open", "high", "low", "close", "volume", "adjusted_close", "date", "ticker")
  
  return(df)
  
}

vix <- get_yahoo('^VIX') %>%
  select(date, vix = adjusted_close) %>% as_tibble() %>% map_df(na.locf0)

bitcoin_model <- bitcoin_model %>% 
  left_join(vix)

rm(vix)
```

```{r}
ggplot(bitcoin_model, aes(x = date)) + 
  geom_line(aes(y = close), colour = "blue") + 
  geom_line(aes(y = vix*50), colour = "red")+
  scale_y_continuous(
    name = "BTC Price",
    sec.axis = sec_axis(~./50, name="VIX Index")
  ) 
```

```{r}
ggplot(bitcoin_model %>% filter(date >= "2017-01-01"), aes(x = date)) + 
  geom_line(aes(y = close), colour = "blue") + 
  geom_line(aes(y = vix*150), colour = "red")+
  scale_y_continuous(
    name = "BTC",
    sec.axis = sec_axis(~./150, name="VIX")
  ) 
```

## 3.4 Bitcoin Price Change

```{r}
price_change <- function(close, n = 1, type = 'continuous', Ticker = NA){
  
  close_change <-  matrix(nrow = length(close), ncol = n) %>% as.data.frame()
  
  for(i in 1:n){
  
    close_change[,i] <- ROC(close, n = i, type = type)
    
    if(is.na(Ticker)){
      names(close_change)[i] <- paste0('close_change_',as.character(i),'d')
    }else{
      names(close_change)[i] <- paste0(Ticker,'_close_change_',as.character(i),'d')
    }
    
  }
  return(close_change)
}

#Continuous Price change (Can be changed back to discrete)
close_change <- price_change(bitcoin_model$close, n = 90, type = 'continuous')

bitcoin_model <- bitcoin_model %>%
  cbind(close_change)

rm(close_change)
```

```{r}
ggplot(bitcoin_model, aes(x = close_change_2d, fill = future_return_sign)) + 
  geom_histogram(binwidth = 0.0025, alpha = 0.5, position = "identity") + 
  coord_cartesian(xlim = c(-0.20, 0.20))
```
```{r}
ggplot(bitcoin_model, aes(x = close_change_2d, y = future_return)) + 
  geom_point(alpha = 0.25, colour = "blue") + 
  coord_cartesian(xlim = c(-0.20, 0.20), ylim = c(-0.10, 0.10)) + 
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = 0) + 
  geom_smooth(colour = "red")
```
```{r}
ggplot(bitcoin_model, aes(x = close_change_16d, fill = factor(future_return_sign))) + 
  geom_histogram(binwidth = 0.01, alpha = 0.5, position = "identity") + 
  coord_cartesian(xlim = c(-0.50, 0.50))
```
```{r}
ggplot(bitcoin_model, aes(x = close_change_16d, y = future_return)) + 
  geom_point(alpha = 0.25, colour = "blue") + 
  coord_cartesian(xlim = c(-0.50, 0.50), ylim = c(-0.10, 0.10)) + 
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = 0) + 
  geom_smooth(colour = "red")
```
```{r}
ggplot(bitcoin_model, aes(x = close_change_57d, fill = factor(future_return_sign))) + 
  geom_histogram(binwidth = 0.01, alpha = 0.5, position = "identity") + 
  coord_cartesian(xlim = c(-0.80, 0.80))
```
```{r}
ggplot(bitcoin_model, aes(x = close_change_57d, y = future_return)) + 
  geom_point(alpha = 0.25, colour = "blue") + 
  coord_cartesian(xlim = c(-0.50, 0.50), ylim = c(-0.10, 0.10)) + 
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = 0) + 
  geom_smooth(colour = "red")
```

## 3.4.1 Momentum of Price Movement

Based on the rate of change from the close price and the price one day before, we are then able to get the momentum of the price movements.

```{r}
# this is finding the derivative of the ROC above
momentum <- function(days_lag){
  
  col <- paste0("close_change_", as.character(days_lag),"d")
  
  df <- bitcoin_model %>%
    select("close", col) %>% 
    mutate(lag_change = lag(bitcoin_model[[col]])) %>%
    mutate(momentum = (bitcoin_model[[col]] - lag_change) / days_lag)
  
  return(df$momentum)
  
}

v <- 1 # can change from 1:90
names(v) <- paste0("momentum_", as.character(v),"d")

bitcoin_model <- bitcoin_model %>% 
  cbind(map_df(v, momentum))
```

We proceed to visualise the momentum to the future_return_sign again to see if there may be a correlation.

```{r}
ggplot(bitcoin_model, aes(x = momentum_1d, fill = future_return_sign)) + 
  geom_histogram(binwidth = 0.0025, alpha = 0.5, position = "identity") + 
  coord_cartesian(xlim = c(-0.20, 0.20))
```
```{r}
ggplot(bitcoin_model, aes(x = momentum_1d, y = future_return)) + 
  geom_point(alpha = 0.25, colour = "blue") + 
  coord_cartesian(xlim = c(-0.20, 0.20), ylim = c(-0.10, 0.10)) + 
  geom_hline(yintercept = 0) + 
  geom_vline(xintercept = 0) + 
  geom_smooth(colour = "red")
```


## 3.4.2 Alternative Asset Class Data

Since BTC exploded in the early 2010s, the role of it has always been widely debated. BTC can be viewed as a safe haven asset class, a risk-on investment or a hedge for the dollar in different periods of its rise. As such, we will use 3 asset classes that represent each potential role that BTC can play. The gold, for safe haven, the S&P 500 as a risk on investment and the dollar itself. 

We have also decided to include the USDCNY currency pair in light of the recent digital yuan drive in China. 

We first scrape the internet for the relevant data.  

```{r}
## Gold 
Gold <- Quandl("LBMA/GOLD") %>% select(1:2)
names(Gold)[names(Gold) == "USD (AM)"] <- "close"
names(Gold)[names(Gold) == "Date"] <- "date"

## DXY 
DXY <- get_yahoo('DX-Y.NYB') %>%
  select(date, close = adjusted_close)

## SP500
SP500 <- get_yahoo('^GSPC') %>%
  select(date, close = adjusted_close)

## USD/Yuan
USDCNY <- get_yahoo('USDCNY=X') %>%
  select(date, close = adjusted_close)
```

Then proceed to find their daily returns and lag them:  

```{r}
lagging_periods=90

## DXY
  DXY[DXY == 0] <- NA
  DXY <- DXY %>%
  map_df(na.locf)
  
  ## Find Daily Returns
  DXY <- DXY %>%
  tq_mutate(select = close,
            mutate_fun = periodReturn,
            period = 'daily',
            type = 'arithmetic',
            col_rename = 'future_return') 

  DXY <- DXY[-1, ]
  
  ## Lagging
  Ticker_name <- deparse(substitute(DXY))
  
  close_change <-  price_change(DXY$close, n = lagging_periods, type = 'discrete', Ticker = Ticker_name) 
  #Can be changed to continuous
  

DXY <- DXY %>%
  cbind(close_change) %>% select(-c(2:3))

## Gold
  Gold[Gold == 0] <- NA
  Gold <- Gold %>%
  map_df(na.locf)
  
  ## Find Daily Returns
  Gold <- Gold %>%
  tq_mutate(select = close,
            mutate_fun = periodReturn,
            period = 'daily',
            type = 'arithmetic',
            col_rename = 'future_return') 

  Gold <- Gold[-1, ]
  
  ## Lagging
  Ticker_name <- deparse(substitute(Gold))
  
  close_change <-  price_change(Gold$close, n = lagging_periods, type = 'discrete', Ticker = Ticker_name)   #Can be changed to continuous
  

Gold <- Gold %>%
  cbind(close_change) %>% select(-c(2:3))

## DXY
  USDCNY[USDCNY == 0] <- NA
  USDCNY <- USDCNY %>%
  map_df(na.locf)
  
  ## Find Daily Returns
  USDCNY <- USDCNY %>%
  tq_mutate(select = close,
            mutate_fun = periodReturn,
            period = 'daily',
            type = 'arithmetic',
            col_rename = 'future_return') 

  USDCNY <- USDCNY[-1, ]
  
  ## Lagging
  Ticker_name <- deparse(substitute(USDCNY))
  
  close_change <-  price_change(USDCNY$close, n = lagging_periods, type = 'discrete', Ticker = Ticker_name) 
  #Can be changed to continuous

USDCNY <- USDCNY %>%
  cbind(close_change) %>% select(-c(2:3))

## DXY
  SP500[SP500 == 0] <- NA
  SP500 <- SP500 %>%
  map_df(na.locf)
  
  ## Find Daily Returns
  SP500 <- SP500 %>%
  tq_mutate(select = close,
            mutate_fun = periodReturn,
            period = 'daily',
            type = 'arithmetic',
            col_rename = 'future_return') 

  SP500 <- SP500[-1, ]
  
  ## Lagging
  Ticker_name <- deparse(substitute(SP500))
  
  close_change <-  price_change(SP500$close, n = lagging_periods, type = 'discrete', Ticker = Ticker_name) 
  #Can be changed to continuous

SP500 <- SP500 %>%
  cbind(close_change) %>% select(-c(2:3))
  
```

Finally we join them to the model:  

```{r}
bitcoin_model <-left_join(bitcoin_model, Gold, by='date',all.x=F) %>%
          left_join(., DXY, by='date') %>% 
          left_join(.,USDCNY,by='date') %>% 
          left_join(.,SP500,by='date')

# Last observation carry forward for weekend data
bitcoin_model <- bitcoin_model %>% mutate_if(is.numeric,na.locf0)

rm(DXY, USDCNY, SP500, Ticker_name,lagging_periods)
```


### 3.4.3 Stock-to-Flow (S2F) Multiple  

The S2F ratio measures the **scarcity** of the asset, taking the **current size of BTC** in the market as a ratio to the **yearly production of BTC**. The S2F model developed by [PlanB](https://twitter.com/100trillionUSD?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor). Although there have been rising doubts around this model for predicting BTC prices, the model has been historically successful in the past few years. The pattern of the spot price approaching the fair value projected with S2F has been respected thus far.  

The multiple takes the ratio of the spot price against the fair value as projected by S2F, i.e. a multiple nearer to 1 would indicate that BTC prices are aligned to price projections due to scarcity; a multiple higher than 1 would likely indicate that BTC is currently overvalued based on its scarcity.  

With growing interest from financial institutions, but not yet from the mass public (as seen from Google Trends), the scarcity of BTC is only going to be greater. Hence, we would like to include this as a feature to measure how scarcity affects the spot price.
 
```{r}
require(jsonlite)
SF_date <- jsonlite::fromJSON("stock-to-flow-ratio.json") %>% 
  as.data.frame() %>% 
  mutate(
    "Date1" = as.POSIXct(t, format="%Y-%m-%dT%H:%M:%S", tz = "UTC"), 
    "date" = as.Date(Date1, format = "%Y-%m-%d")) %>% 
  select(date)

SF_values <- jsonlite::fromJSON("stock-to-flow-ratio.json") %>% as.data.frame() %>% select(o)
SF_values <- unnest(SF_values$o)

S2F_data <- cbind(SF_date, SF_values) %>% select(-daysTillHalving) %>% rename("S2Fratio" = "ratio")
rm(SF_date, SF_values)

bitcoin_model <- bitcoin_model %>% 
  left_join(S2F_data, by = "date") %>% 
  mutate(S2F_multiple = close/S2Fratio) %>% 
  filter(!is.na(S2F_multiple)) %>% 
  select(-S2Fratio)

rm(S2F_data)
```

### 3.4.4 Bitcoin Miner Revenue  

In this section, we will focus more on miners' revenue. Miners' revenue are heavily correlated with BTC price, however, we also want to see if the rate of change of revenue and momentum plays a part in determining BTC price.  

We also want to see if the moving average (MA) of miners' revenue would introduce any pattern that can be picked up by the model. `frollmean()` is another useful function that makes calculating a set of `1:X`-MA (where X is a positive integer) easier.  

```{r}
# bitcoin mining revenue
bc_mine_rev <- Quandl("BCHAIN/MIREV") %>% 
  arrange(Date) %>%
  rename(miner_rev = Value)

# mining revenue 1-90 day lag
bc_mine_rev_lag <- shift(bc_mine_rev$miner_rev, n = 1:90, type = 'lag', give.names = TRUE) %>% 
  as.data.frame() %>% 
  rename_all(funs(paste0(gsub('V1_lag_', 'miner_rev_lag_', x = .), 'd')))

# mining revenue ROC
bc_mine_rev_change <- bc_mine_rev %>%
  cbind(bc_mine_rev_lag) %>%
  rename_all(funs(gsub('lag', 'change', x = .))) %>%
  mutate_if(grepl('miner_rev_change', names(.)), ~ ifelse(. == 0, 0, (miner_rev - .)/ .))

# mining revenue 2nd derive
bc_mine_rev_change2 <- bc_mine_rev_change %>%
  rename_all(funs(gsub('change', 'change2', x = .))) %>%
  mutate_if(grepl('miner_rev_change2', names(.)), 
            ~ ifelse(lag(., n = 1L) == 0, 0, 
                     (. - lag(., n = 1L)) / lag(., n = 1L)))

# mining revenue ma 5/10/15/20/25/30/35/40/45/50
bc_mine_rev_ma <- frollmean(bc_mine_rev$miner_rev, seq(5, 50, 5), align = 'right') %>%
  as.data.frame()
names(bc_mine_rev_ma) <- paste0(sprintf('bc_mine_rev_ma%s', seq(5, 50, 5)), 'd')

# combine all features
bc_mine_rev <- bc_mine_rev %>% 
  cbind(bc_mine_rev_lag, 
        bc_mine_rev_ma) %>%
  inner_join(bc_mine_rev_change, by = c('Date', 'miner_rev')) %>%
  inner_join(bc_mine_rev_change2, by = c('Date', 'miner_rev'))

# mining revenue drawdown
bc_mine_rev <- bc_mine_rev %>%
  mutate(miner_rev_drawdown = -1 * (1 - miner_rev / cummax(miner_rev)))

bitcoin_model <- bitcoin_model %>%
  left_join(bc_mine_rev, by = c('date' = 'Date'))

rm(bc_mine_rev, bc_mine_rev_change, bc_mine_rev_change2, bc_mine_rev_lag, bc_mine_rev_ma)
```

### 3.4.5 Techfactors

```{r}
# devtools::install_github("shrektan/techfactor")
library(techfactor)

#define function to get alpha
get_alpha <- function(df_1, df_2, n ='all',baseline = 'SP500'){
  #Techfactor only work with uppcase
  df_1 <- df_1 %>% rename_all(toupper)
  df_2 <- df_2 %>% rename_all(toupper)
  
  # Identify key as date
  df_1 <- data.table(df_1,key='DATE')
  df_2 <- data.table(df_2,key='DATE')
  
  tf_quote <- df_1[df_2] %>% rename(BMK_CLOSE = i.CLOSE,
                                        BMK_OPEN = i.OPEN)
  # \code{BMK_OPEN} is the close and open price data of the index
  # https://github.com/shrektan/techfactor/blob/master/man/tf_quote.Rd
  # https://github.com/shrektan/techfactor
  tf_quote <- tf_quote[-1,]
  
  #change 0 in pclose to na for TF to work
  tf_quote[tf_quote == 0] <- NA
  
  #get alpha
  from_to <- range(tf_quote$DATE)
  alpha_df <- tf_quote %>% column_to_rownames(var = 'DATE') %>% select(CLOSE)
  factors <- tf_reg_factors()
  
  if(n != 'all'){
    for(i in n:n){
      normal_factor <- attr(factors, "normal")[i]
      qt <- tf_quote_xptr(tf_quote)
      alpha <- tf_qt_cal(qt, normal_factor, from_to) %>% as_tibble()
      alpha_df <- alpha_df %>%cbind(alpha)
      print(paste('Working on', names(alpha)))
    }
  }else{
    for(i in 1:128){
      #128 is ALL 191 factors
      normal_factor <- attr(factors, "normal")[i]
      qt <- tf_quote_xptr(tf_quote)
      alpha <- tf_qt_cal(qt, normal_factor, from_to) %>% as_tibble()
      alpha_df <- alpha_df %>%cbind(alpha)
      print(paste('Working on', names(alpha)))
    }
  }
  
  #Add in alpha_df as new features
  alpha_df <- alpha_df %>% select(-CLOSE) %>% 
  rownames_to_column(var = 'date') %>% 
  mutate(date = ymd(date)) %>% rename_if(is.numeric, funs(paste0(baseline ,gsub('alpha', '_alpha_', x = .))))

  return(alpha_df)
}
```

```{r, message=FALSE, results=FALSE}
#BTC price information
bitcoin_p <- bitcoin_price %>% select(-volume_currency) %>% 
  mutate(pclose = lag(close),
         amount = weighted_price*volume_btc) %>%
  rename(vwap = weighted_price,
         volume = volume_btc) %>% 
  select(date,pclose,open,high,low,close,vwap,volume,amount)
  
#Remove first row
bitcoin_p <- bitcoin_p[-1,]

#Index information
SP500 <- tq_get('^GSPC',
                        from = "2011-09-13",
                        to = Sys.Date(),
                        get = "stock.prices")  

SP500 <- SP500 %>% select(date,close,open)

alpha_df <- get_alpha(bitcoin_p,SP500, n = 'all', baseline = 'SP500')

bitcoin_model <- bitcoin_model %>% left_join(alpha_df, by = c("date"),all.x=F)
#what each alpha means can be found here, https://arxiv.org/ftp/arxiv/papers/1601/1601.00991.pdf

rm(SP500, bitcoin_p, alpha_df)
```

## 3.5 Bitcoin Drawdown

```{r}
bitcoin_model <- bitcoin_model %>%
  mutate(close_drawdown = -1 * (1 - close / cummax(close)))
```

```{r}
ggplot(bitcoin_model, aes(x = close_drawdown, fill = factor(future_return_sign))) + 
  geom_histogram(binwidth = 0.01, alpha = 0.5, position = "identity")
```

## 3.6 Rolling Daily Return Volatility

```{r}
close_sd <- frollapply(bitcoin_model$close_change_1d, 1:90, sd) %>%
  as.data.frame()
names(close_sd) <- paste0(sprintf('close_sd_%s', seq(1:90)), 'd')

bitcoin_model <- bitcoin_model %>%
  cbind(close_sd) %>%
  mutate(close_sd_1d = 0)

rm(close_sd)
```

```{r}
ggplot(bitcoin_model, aes(x = close_sd_10d, fill = factor(future_return_sign))) +
  geom_histogram(binwidth = 0.001, position = "identity", alpha = 0.5)
```

```{r}
ggplot(bitcoin_model, aes(x = close_sd_90d, fill = factor(future_return_sign))) + 
  geom_histogram(binwidth = 0.001, position = "identity", alpha = 0.5)
```

## 3.7 Number of Positive Days

```{r}
bitcoin_model <- bitcoin_model %>%
  mutate(close_positive = ifelse(close_change_1d > 0, 1, 0), 
         close_negative = ifelse(close_change_1d <= 0, 1, 0))

close_positive <- frollsum(bitcoin_model$close_positive, 1:90, align = 'right') %>%
  as.data.frame()
names(close_positive) <- paste0(sprintf('close_positive_%s', seq(1:90)), 'd')

bitcoin_model <- bitcoin_model %>%
  cbind(close_positive)

rm(close_positive)
```

## 3.8 Number of Consecutive Positive and Negative Days

```{r}
bitcoin_model <- bitcoin_model %>% 
  mutate(close_positive_streak = close_positive * unlist(map(rle(close_positive)[["lengths"]], seq_len)), 
         close_negative_streak = close_negative * unlist(map(rle(close_negative)[["lengths"]], seq_len)))
```

```{r}
ggplot(bitcoin_model, aes(x = close_positive_streak, fill = factor(future_return_sign))) + 
  geom_histogram(binwidth = 1, position = "identity", alpha = 0.5)
```

```{r}
ggplot(bitcoin_model, aes(x = close_negative_streak, fill = factor(future_return_sign))) + 
  geom_histogram(binwidth = 1, position = "identity", alpha = 0.5)
```

```{r}
ggplot(bitcoin_model, aes(x = close_positive_26d, fill = factor(future_return_sign))) + 
  geom_histogram(binwidth = 1, position = "identity", alpha = 0.5)
```

```{r}
ggplot(bitcoin_model, aes(x = close_positive_63d, fill = factor(future_return_sign))) + 
  geom_histogram(binwidth = 1, position = "identity", alpha = 0.5)
```


## 3.9 Time Series Features

We have added the following time series features:

* **entropy**: Measures the "forecastability" of a series - low values = high sig-to-noise, large vals = difficult to forecast
* **stability**: Means/variances are computed for all tiled windows - stability is the variance of the means
* **lumpiness**: Lumpiness is the variance of the variances
* **max_level_shift**: Finds the largest mean shift between two consecutive windows (returns two values, size of shift and time index of shift)
* **max_var_shift**: the max variance shift between two consecutive windows (returns two values, size of shift and time index of shift)
* **max_kl_shift**: Finds the largest shift in the Kulback-Leibler divergence between two consecutive windows (returns two values, size of shift and time index of shift)
* **crossing_points**: Number of times a series crosses the mean line


```{r}
ts_feature_set <- c(
    "entropy", 
    "stability", 
    "lumpiness",     
    "max_level_shift",
    "max_var_shift",  
    "max_kl_shift", 
    "crossing_points" 
)

bitcoin_model <- bitcoin_model %>%
  mutate(ts_features = slide(
    .x = future_return,
    .f = ~ tsfeatures(.x, features = ts_feature_set),
    .before = 90,
    .complete = TRUE)) %>% 
  unnest(ts_features)
```

## 3.10 Technical Indicators  

We will be highlighting the below indicators that form our trading strategy:  

* 20, 50 & 200 Day Exponential Moving Average (EMA): an indicator for entry/exit when yesterday's and today's candle are above/below EMA.  
* Golden Cross: flashes a __buy__ signal when the current 20-day EMA crosses above the current 50-day EMA and a __sell__ signal when the current 50-day EMA crosses under the current 20-day EMA.  
* Moving Average Convergence Divergence (MACD): a signal cross over after a day's candle to indicate trend-following momentum.  
* Stochastic Momentum Index (SMI): another typical indicator for momentum, using the 20-day EMA.  

These indicators will indicate a binary output which will be used as features for our final model.  

```{r}
bitcoin_TI <- bitcoin_price %>% select(date, close)
```

### 3.10.1 Exponential Moving Average (EMA)  

```{r}
#20-day EMA
bitcoin_TI <- bitcoin_TI %>% mutate(ema20 = EMA(
  close, 
  n = 20
)) %>% mutate(ema_20P = Lag( 
  ifelse(
    Lag(close, 2) < Lag(ema20, 2) & Lag(close) > Lag(ema20) & close > ema20, 
    # previous day ema > close, yesterday close > ema and today close > ema
    1, 
    ifelse(
      Lag(close, 2) > Lag(ema20, 2) & Lag(close) < Lag(ema20) & close < ema20,
      #previous day close > ema, yesterday close < ema and today close < ema
      0, 
      NA))
  )
) %>% fill(ema_20P, .direction = "down") %>% 
 mutate(ema_20P = ifelse(is.na(ema_20P) == T, 0, ema_20P))

#50-day EMA
bitcoin_TI <- bitcoin_TI %>% mutate(ema50 = EMA(
  close, 
  n = 50
)) %>% mutate(ema_50P = Lag( 
  ifelse(
    Lag(close, 2) < Lag(ema50, 2) & Lag(close) > Lag(ema50) & close > ema50, 
    1, 
    ifelse(
      Lag(close, 2) > Lag(ema50, 2) & Lag(close) < Lag(ema50) & close < ema50, 
      0, 
      NA))
  )
) %>% fill(ema_50P, .direction = "down") %>% 
  mutate(ema_50P = ifelse(is.na(ema_50P) == T, 0, ema_50P))

#200-day EMA
bitcoin_TI <- bitcoin_TI %>% mutate(ema200 = EMA(
  close,
  n=200
)) %>% mutate(ema_200P = Lag(
  ifelse(
    Lag(close, 2) < Lag(ema200, 2) & Lag(close) > Lag(ema200) & close > ema200, 
    1, 
    ifelse(
      Lag(close, 2) > Lag(ema200, 2) & Lag(close) < Lag(ema200) & close < ema200,
      0,
      NA)
    )
  )
) %>% fill(ema_200P, .direction = "down") %>% 
  mutate(ema_200P = ifelse(is.na(ema_200P) == T, 0, ema_200P))
```

### 3.10.2 Golden Cross  
```{r}
#Cross-over of ema 20 and ema 50
bitcoin_TI <- bitcoin_TI %>% mutate(ema_CO_2050 = Lag(
  ifelse(Lag(ema20) < Lag(ema50) & ema20 > ema50,
         1,
         ifelse(
           Lag(ema20) > Lag(ema50) & ema20 < ema50,
           0, NA
         )
      )
  )
) %>% fill(ema_CO_2050, .direction = "down") %>% 
  mutate(ema_CO_2050 = ifelse(is.na(ema_CO_2050) == T, 0, ema_CO_2050)) 
```


### 3.10.3 Moving Average Convergence Divergence (MACD)  

```{r}
# * MACD (12,26,9) ----
macd <- MACD(
  Cl(bitcoin_price), 
  nFast = 12, 
  nSlow = 26, 
  nSig = 9
)

bitcoin_TI <- bitcoin_TI %>% cbind(
                    Lag(
  ifelse(
    Lag(macd[, 1]) < Lag(macd[, 2]) & macd[, 1] > macd[, 2],
    1,
    ifelse(
        Lag(macd[, 1]) > Lag(macd[, 2]) & macd[, 1] < macd[, 2], 
        0,NA
      )        
    )      
  ) %>% as.data.frame() %>% 
  fill(Lag.1, .direction = "down")
) %>% rename("macd_CO" = "Lag.1") %>% mutate(macd_CO = ifelse(is.na(macd_CO)== T, 0, macd_CO))

rm(macd)
```

### 3.10.4 Stochastic Momentum Index (SMI)  
Using 20-day EMA as a baseline:  

```{r}
smi <- SMI(
  cbind(
    Hi(bitcoin_price), 
    Lo(bitcoin_price), 
    Cl(bitcoin_price)
  ),
  n = 13,
  nFast = 2,
  nSlow = 25,
  nSig = 9
)

bitcoin_TI <- bitcoin_TI %>% mutate(smi_value = smi[,1]) %>% mutate(SMI = 
  Lag(
    ifelse(Lag(close) < Lag(ema20) & close > ema20 & smi_value < -40,
      1,
      ifelse(Lag(close) > Lag(ema20) & close < ema20 & smi_value > 40,
             0, NA)
    )
  )
) %>% fill(SMI, .direction = "down") %>% mutate(SMI = ifelse(is.na(SMI) == T, 0, SMI))

rm(smi)
```

## 3.11 Cleaning Data  

Note that our `technical indicator` features are binary, and thus they will be treated as categorical variables.  

We will also include typical `bitcoin indicators` as listed in [Section 2.3](#Section23). 

```{r}
bitcoin_model <- bitcoin_model %>% select(-matches('close_sd_')) 

bitcoin_TI <- bitcoin_TI %>% select(-c(ema50, ema20, ema200, smi_value)) 

bitcoin_model <- bitcoin_model %>% left_join(bitcoin_TI, by = c("date", "close"))
bitcoin_model <- bitcoin_model %>% left_join(bitcoin_data, by = "date")

bitcoin_model <- bitcoin_model %>% map_df(~na.locf0(.))
bitcoin_model <- bitcoin_model %>% filter(date >= '2017-01-02') #first mon of 2017

rm(bitcoin_TI, bitcoin_data)
```


<div id = "Section4"></div>
# 4. Predictive Modeling  

After we have joined the features into our `bitcoin_model` dataframe, we can now begin to build our predictive models. We will first proceed to introduce the models that will be used, namely:  

* XGBoost  
* Catboost  
* Logistic Regression  
* Light-Gradient Boost (LGB) - which will not be evaluated due to limited hardware on our end, but its steps are detailed in [Appendix A](#AppendixA)  

The `tidymodels` package allows us to create our analysis based on a series of steps. 

We first have to replace the NAs within our dataset, which are from features (such as **VIX**) that are closed on weekends. We will be using the `na.locf` to deal with NAs.  

```{r}
bitcoin_model <- bitcoin_model %>% map_df(~na.locf0(.))
```

## 4.1 Preparation for Modelling

### 4.1.1 Initial Data Split

```{r}
data_split <- rsample::initial_time_split(bitcoin_model,prop=0.8)
train <- training(data_split)
test <- testing(data_split)
```


### 4.1.2 Recipe

The first step would be to create a `recipe`, which acts as a pipeline which the models we choose to use can be built upon by specifying in `set_engine()`.

```{r}
recipe_spec <- recipe(future_return_sign ~ ., data = train) %>%
  update_role(date, future_return, close, new_role = "ID")

rmarkdown::paged_table(recipe_spec %>% prep() %>% juice() %>% head())
```

The chosen models can then be easily cross validated by piping `time_series_cv()` and `tuning` can also be easily piped with `tuneGrid()`.

### 4.1.3 Cross Validation

For time series cross validation, we recognize that they are 2 methods that we can do. The expanding window method and the sliding window method. Conveniently, using the `modeltime` package we can easily switch between the 2 methods using `cumulative = F/T`.  

```{r}
resamples_cv_expanding <- recipe_spec %>%
  prep() %>%
  juice() %>%
  time_series_cv(
    date_var = date,
    initial = '3 month',
    assess = '3 month',
    skip = '3 month',
    cumulative = TRUE
  )

# resamples_cv_sliding <- recipe_spec %>%
#   prep() %>%
#   juice() %>%
#   time_series_cv(
#     date_var = date,
#     initial = '3 month',
#     assess = '3 month',
#     skip = '3 month',
#     cumulative = FALSE
#   )
```

After comparing the results from both methods, the **expanding** method yielded better results.  

## 4.2 XGBoost

### 4.2.1 Defining Workflow

```{r}
xgb_model <- boost_tree(learn_rate = 0.01,
                        tree_depth = 1,
                        min_n = 1,
                        mtry = 500,
                        trees = tune(),
                        stop_iter = 50) %>%
  set_engine('xgboost') %>%
  set_mode('classification')

xgb_model
```

```{r}
xgb_wflw <- workflow() %>%
  add_recipe(recipe_spec) %>%
  add_model(xgb_model)

xgb_wflw
```

### 4.2.2 Training Model

```{r}
library(doParallel)
all_cores <- parallel::detectCores(logical=F)
registerDoParallel(cores = all_cores)

xgb_params <- grid_max_entropy(trees(), size = 10)

xgb_model_trained <- tune_grid(
  xgb_wflw, 
  grid = xgb_params,
  metrics = metric_set(mn_log_loss),
  resamples = resamples_cv_expanding, #expanding is used instead of sliding
  control = control_resamples(verbose = FALSE,
                              save_pred = TRUE,
                              allow_par = TRUE))

xgb_model_trained %>% collect_metrics()
```

### 4.2.3 Making Predictions  

After training the model, we can use `select_best` to choose our best model and `finalize` based on a specified metric. The team has decided to specify with **mean logarithmic loss**, to see which model yields the best predictions. 

The reason that the team has decide to use the **mean logarithmic loss** is because we wish to go beyond just the final class prediction and evaluate the absolute probabilistic difference of each prediction. The more certain our model is that an observation is 1 for example, which it is in fact, the lower the error. Conversely, it penalizes very heavily when the model is very certain about an outcome that is untrue.

Once we have finalized the model, we can test our predictions using the test set. We can simply `bake` the recipes and indicate a `new_data` to implement the finalised model on different train/test sets.

```{r}
best_params <- xgb_model_trained %>%
  select_best('mn_log_loss', maximise = FALSE)

xgb_model_best <- xgb_model %>%
  finalize_model(best_params)

xgb_wflw_best <- workflow() %>%
  add_recipe(recipe_spec) %>%
  add_model(xgb_model_best)
```

### 4.2.4 Evaluating Performance 

```{r}
#Performance on Training 
train_processed <- bake(recipe_spec %>% prep(),  new_data = train)

train_prediction_xgb <- xgb_model_best %>%
  fit(
      formula = future_return_sign ~ .,
      data = train_processed
  ) %>% 
  predict(new_data = train_processed,type='prob') %>% 
  bind_cols(train) %>% mutate(.pred_class=ifelse(.pred_0>0.5,0,1))

metrics_list = metric_set(mn_log_loss)

xgboost_score_train <- train_prediction_xgb %>% 
  metrics_list(truth=future_return_sign,estimate=.pred_class,.pred_0)

knitr::kable(xgboost_score_train)

#Performance on Testing
test_processed <- bake(recipe_spec %>% prep(),  new_data = test)

test_prediction_xgb <- xgb_model_best %>%
  fit(
      formula = future_return_sign ~ .,
      data = train_processed
  ) %>% 
  predict(new_data = test_processed,type='prob') %>% 
  bind_cols(test) %>% mutate(.pred_class=ifelse(.pred_0>0.5,0,1))

xgboost_score_test <- test_prediction_xgb %>% metrics_list(future_return_sign,estimate=.pred_class,.pred_0)

knitr::kable(xgboost_score_test)
```

## 4.3 CatBoost

```{r}
# install.packages('devtools')
# devtools::install_url('https://github.com/catboost/catboost/releases/download/v0.24.3/catboost-R-Windows-0.24.3.tgz', INSTALL_opts = c("--no-multiarch"))
set_dependency("boost_tree", eng = "catboost", "catboost")
set_dependency("boost_tree", eng = "catboost", "treesnip")
```


### 4.3.1 Defining Model

```{r}
cat_model <- boost_tree(learn_rate = 0.01,
                        tree_depth = 1,
                        min_n = 1,
                        mtry = 500,
                        trees = tune(),
                        stop_iter = 50) %>%
  set_engine('catboost') %>%
  set_mode('classification')

cat_model
```

```{r}
cat_wflw <- workflow() %>%
  add_recipe(recipe_spec) %>%
  add_model(cat_model)

cat_wflw
```

### 4.3.2 Training Model

```{r}
cat_params <- grid_max_entropy(trees(), size = 10)

cat_model_trained <- tune_grid(
  cat_wflw, 
  grid = cat_params,
  metrics = metric_set(mn_log_loss), 
  resamples = resamples_cv_expanding,
  control = control_resamples(verbose = FALSE,
                              save_pred = TRUE,
                              allow_par = TRUE))

cat_model_trained %>% collect_metrics()
```


### 4.3.3 Making Predictions  

```{r}
best_params <- cat_model_trained %>%
  select_best('mn_log_loss', maximise = FALSE)

cat_model_best <- cat_model %>%
  finalize_model(best_params)

cat_wflw_best <- workflow() %>%
  add_recipe(recipe_spec) %>%
  add_model(cat_model_best)

```

### 4.3.4 Evaluating Performance 

```{r}
#Performance on Training 
train_processed <- bake(recipe_spec %>% prep(),  new_data = train)

train_prediction_cat <- cat_model_best %>%
  fit(
      formula = future_return_sign ~ .,
      data = train_processed %>% select(-date)
  ) %>% 
  predict(new_data = train_processed,type='prob') %>% 
  bind_cols(train) %>% mutate(.pred_class=ifelse(.pred_0>0.5,0,1))

metrics_list = metric_set(mn_log_loss)

cat_score_train <- train_prediction_cat %>% 
  metrics_list(truth=future_return_sign,estimate=.pred_class,.pred_0)

knitr::kable(cat_score_train)

#Performance on Testing
test_processed <- bake(recipe_spec %>% prep(),  new_data = test)

test_prediction_cat <- cat_model_best %>%
  fit(
      formula = future_return_sign ~ .,
      data = train_processed %>% select(-date)
  ) %>% 
  predict(new_data = test_processed,type='prob') %>% 
  bind_cols(test) %>% mutate(.pred_class=ifelse(.pred_0>0.5,0,1))

cat_score_test <- test_prediction_cat %>% metrics_list(future_return_sign,estimate=.pred_class,.pred_0)

knitr::kable(cat_score_test)

```


## 4.4 Logistic Regression  

### 4.4.1 Defining Recipe  

By setting `mixture = 1`, it helps to remove irrelevant predictors and choose a simpler model.

```{r}
log_model <- logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")

log_wflw <- workflow() %>% add_model(log_model) %>% 
  add_recipe(recipe_spec)
```

### 4.4.2 Training Model

```{r}
log_params <- grid_max_entropy(parameters(penalty()), size = 10)

all_cores <- parallel::detectCores(logical=F)
registerDoParallel(cores = all_cores)

log_model_trained <- tune_grid(
   log_wflw, 
   grid = log_params,
   metrics = metric_set(mn_log_loss),
   resamples = resamples_cv_expanding,
   control = control_resamples(verbose = T,
                               save_pred = TRUE,
                               allow_par = TRUE))
 
log_model_trained %>% collect_metrics()
```

### 4.4.3 Making Predictions  

```{r}
best_params <- log_model_trained %>%
   select_best('mn_log_loss', maximise = FALSE)

log_model_best <- log_model %>%
   finalize_model(best_params)
 
log_wflw_best <- workflow() %>%
   add_recipe(recipe_spec) %>%
   add_model(log_model_best)
```

### 4.4.4 Evaluating Performance 

```{r}
#Performance on Training 
train_processed <- bake(recipe_spec %>% prep(),  new_data = train)
 
train_prediction_log <- log_model_best %>%
   fit(
       formula = future_return_sign ~ .,
       data = train_processed %>% select(-date)
   ) %>% 
  predict(new_data = train_processed,type='prob') %>% 
  bind_cols(train) %>% mutate(.pred_class=ifelse(.pred_0>0.5,0,1))
 
metrics_list = metric_set(mn_log_loss)
 
log_score_train <- train_prediction_log %>% 
   metrics_list(truth=future_return_sign,estimate=.pred_class,.pred_0)
 
knitr::kable(log_score_train)

 
#Performance on Testing
test_processed <- bake(recipe_spec %>% prep(),  new_data = test)
 
test_prediction_log <- log_model_best %>%
   fit(
       formula = future_return_sign ~ .,
       data = train_processed %>% select(-date)
   ) %>% 
   predict(new_data = test_processed,type='prob') %>% 
   bind_cols(test) %>% mutate(.pred_class=ifelse(.pred_0>0.5,0,1))
 
log_score_test <- test_prediction_log %>% metrics_list(future_return_sign,estimate=.pred_class,.pred_0)
 
knitr::kable(log_score_test)

```

## 4.5 Model Summaries (on Train and Test Set)  

```{r}
model_summaries <- rbind(
  xgboost_score_train, 
  xgboost_score_test, 
  cat_score_train, 
  cat_score_test, 
  log_score_train, 
  log_score_test) 

model_summaries$Model <- c(rep("XGBoost", 2), rep("Catboost", 2), rep("Log Reg", 2))
model_summaries$Data <- rep(c("Train", "Test") , 3) 

model_summaries <- model_summaries %>% select(Model, Data, .metric, .estimate) %>% arrange(.estimate)

knitr::kable(model_summaries)
```

## 4.6 Model Performances (on Test Set)

```{r}
stratcomp_xgb <- test_prediction_xgb %>% mutate(.pred_class = as.numeric(.pred_class)) %>% 
  mutate(`Buy & Hold` = ROC(close, n = 1) %>% lead(), 
         `XGB Strategy` = .pred_class*`Buy & Hold`) %>%
  select(date, `Buy & Hold`, `XGB Strategy`)

stratcomp_cat <- test_prediction_cat %>% mutate(.pred_class = as.numeric(.pred_class)) %>% 
  mutate(`Buy & Hold` = ROC(close, n = 1) %>% lead(), 
         `Cat Strategy` = .pred_class*`Buy & Hold`) %>%
  select(date, `Buy & Hold`, `Cat Strategy`)

stratcomp_log <- test_prediction_log %>% mutate(.pred_class = as.numeric(.pred_class)) %>% 
  mutate(`Buy & Hold` = ROC(close, n = 1) %>% lead(), 
         `Log Strategy` = .pred_class*`Buy & Hold`) %>%
  select(date, `Buy & Hold`, `Log Strategy`)

stratcomp <- stratcomp_xgb %>%
  left_join(stratcomp_cat, by = c("date", "Buy & Hold")) %>%
  left_join(stratcomp_log, by = c("date", "Buy & Hold")) %>%
  column_to_rownames(var = "date") %>% as.xts()

rm(stratcomp_xgb, stratcomp_cat, stratcomp_log)

table.AnnualizedReturns(stratcomp)
charts.PerformanceSummary(stratcomp, main = "Strategy Performance")
```



### 4.6.1 Model Performances (on Test Set) with Transaction Costs  

Factoring a 0.3% transaction cost:

```{r}
transaction_cost = 0.003

stratcomp_xgb_cost <- test_prediction_xgb %>% mutate(
  `Buy & Hold` = ROC(close, n = 1) %>% lead(),
  `XGB Strategy` = .pred_class*`Buy & Hold`,
    # if strategy positive aka .pred_class > 0.5, signal to be 1
  trading_cost = (abs(.pred_class - lag(.pred_class, n = 1, default = 0))*transaction_cost), 
    #if it's 1 - 1 or 0 - 0, no cost. If any change, then there's cost
  `XGB Strategy C` = `XGB Strategy` - trading_cost) %>% 
  select(date, `Buy & Hold`, `XGB Strategy C`)

stratcomp_cat_cost <- test_prediction_cat %>% mutate(
  `Buy & Hold` = ROC(close, n = 1) %>% lead(),
  `Cat Strategy` = .pred_class*`Buy & Hold`,

  trading_cost = (abs(.pred_class - lag(.pred_class, n = 1, default = 0))*transaction_cost),
  `Cat Strategy C` = `Cat Strategy` - trading_cost) %>% 
  select(date, `Buy & Hold`, `Cat Strategy C`) 

stratcomp_log_cost <- test_prediction_log %>% mutate(
  `Buy & Hold` = ROC(close, n = 1) %>% lead(),
  `Log Strategy` = .pred_class*`Buy & Hold`,
  trading_cost = (abs(.pred_class - lag(.pred_class, n = 1, default = 0))*transaction_cost),
  `Log Strategy C` = `Log Strategy` - trading_cost) %>% 
  select(date, `Buy & Hold`, `Log Strategy C`) 


stratcomp_cost <- stratcomp_xgb_cost %>%
  left_join(stratcomp_cat_cost, by = c("date", "Buy & Hold")) %>%
  left_join(stratcomp_log_cost, by = c("date", "Buy & Hold")) %>%
  column_to_rownames(var = "date") %>% as.xts()

rm(stratcomp_xgb_cost, stratcomp_cat_cost, stratcomp_log_cost)

table.AnnualizedReturns(stratcomp_cost)
charts.PerformanceSummary(stratcomp_cost, main = "Strategy (with Cost) Performance")
```

<div id = "Section5"></div>
# 5. Modelling with Feature Selection  

We recognise the overfitting problem without reducing the number of features in the dataset, as we can clearly see in the results, the CV and Test result differ greatly, showing degrees of overfitting. Thus we will proceed by using feature selection methods to mitigate the problem of overfitting.

With `tidymodels`, we can easily create a base recipe to build these different feature selection steps upon.  

```{r}
recipe_select_base <- recipe(future_return_sign ~ ., data = bitcoin_model) %>%
  update_role(date, future_return, close, new_role = "ID") 
```

## 5.1 Through Principal Component Analysis

Given our large dataset, Principal Component Analysis (PCA) allows us to reduce the dimensionality of our dataset, while retaining majority of the information provided by our features. This is done by reducing the features into its __principal components__ that capture the maximal amount of variance in BTC prices.  

PCA entails several steps, firstly needing to standardise the range of continuous features to equalise their contributions. This can be easily done by first normalising our numeric columns with `step_normalize()`. The next step in PCA would be then to obtain the **covariance matrix**, which allows us to identify correlated features.  Lastly, **eigenvectors and eigenvalues** are computed from the covariance matrix to determine the principal components. PCA then tries to allocate maximum possible information to the first component, then the maximum __remaining__ information to the second component and so on.  

```{r}
recipe_select_pca <- recipe_select_base %>% 
  step_nzv(all_predictors()) %>% # removed near zero variance variables
  step_normalize(all_numeric(), - all_outcomes()) %>% 
  step_pca(all_predictors(), num_comp = 10, id = "pca")

pca_estimates <- prep(recipe_select_pca)

pca_info <- tidy(pca_estimates, id = "pca", type = "variance")

ggplot(pca_info %>% filter(terms == "variance"), aes(x=component, y = value)) + 
  geom_line(stat="identity") + geom_point() + 
  labs(x = "Principal Component", y = "Variance", title = "Variance Explained by Each Component")
```

From the above plot, we see that majority of our features do not have significant impact in explaining the variances.  

```{r, message = F}
ggplot(pca_info %>% filter(terms == "variance"), aes(x=component, y = value)) + 
  geom_line(stat="identity") + 
  geom_point() + 
  labs(x = "Principal Component", 
      y = "Variance", 
      title = "Variance Explained by Each Component") + 
  xlim(c(0,30))
```

And we can see that the impact on variance begins to tail off after the 10th principal component.

```{r, message = F}
ggplot(pca_info %>% filter(terms == "cumulative percent variance") %>% top_n(-10, value), 
       aes(x=component, y = value)) + geom_line(stat="identity") + 
  geom_bar(stat="identity", aes(fill = value)) + geom_point() + 
  geom_text(aes(x=component, y = value, label = paste0(round(value, 2), "%")), vjust=-.5, size = 3) + 
  scale_x_discrete(limits = seq(1,10)) + 
  labs(x = "Principal Component", 
       y = "Variance", 
       title = "Cumulative Variance Explained with Each Component", 
       subtitle = "For first 10 principal components") 
```

From the above screeplot, see that across the first 10 principal components, the sum of these only explain up to `r paste0(round(pca_info %>% filter(terms == "cumulative percent variance") %>% filter(component == 10) %>% pull(value), 1), "%")` of the model. 

Thus, we will be looking at the first 10 principal components. 

```{r}
# new recipe
recipe_select_pca <- recipe_select_base %>% 
  step_nzv(all_predictors()) %>% 
  step_normalize(all_numeric(), - all_outcomes()) %>% 
  step_pca(all_predictors(), num_comp = 10, id = "pca")

# dataframe
pca_prep <- juice(prep(recipe_select_pca))

# Train test split pca_prep
pca_split <- rsample::initial_time_split(pca_prep,prop=0.8)
train <- training(pca_split)
test <- testing(pca_split)

rm(pca_estimates, pca_info, recipe_select_pca)

# new recipe
recipe_final_pca <- recipe(future_return_sign ~., data = train) %>% 
  update_role(date, future_return, close, new_role = "ID")

# getting resamples
resamples_pca_expanding <- recipe_final_pca %>% 
  prep() %>% juice() %>% 
  time_series_cv(datevar = date, 
                 nitial = '3 month', 
                 assess = '3 month', 
                 skip = '3 month', 
                 cumulative = T)

## Model of choice
selected_model <- cat_model
selected_params <- cat_params

pca_wflw <- workflow() %>% add_recipe(recipe_final_pca) %>% add_model(selected_model)

pca_trained <- tune_grid(
  pca_wflw,
  grid = selected_params, 
  metrics = metric_set(mn_log_loss), 
  resamples = resamples_pca_expanding,
  control = control_resamples(verbose = F, save_pred = T, allow_par = T)
)
```

After which we will evaluate the performance:  

```{r}
best_pca_params <- pca_trained %>%
  select_best('mn_log_loss', maximise = FALSE)

pca_model_best <- selected_model %>%
  finalize_model(best_pca_params)

pca_wflw_best <- workflow() %>%
  add_recipe(recipe_final_pca) %>%
  add_model(pca_model_best)

train_processed <- bake(recipe_final_pca %>% prep(),  new_data = train)
train_prediction_pca <- pca_model_best %>%
  fit(
      formula = future_return_sign ~ .,
      data = train_processed %>% select(-date)
  ) %>% 
  predict(new_data = train_processed,type='prob') %>% 
  bind_cols(train) %>% mutate(.pred_class=ifelse(.pred_0>0.5,0,1)) %>%
  mutate(.pred_class = as.factor(.pred_class))

metrics_list = metric_set(mn_log_loss)

pca_score_train <- train_prediction_pca %>% 
  metrics_list(truth=future_return_sign,estimate=.pred_class,.pred_0)

knitr::kable(pca_score_train)

#Performance on Testing
test_processed <- bake(recipe_final_pca %>% prep(),  new_data = test)

test_prediction_pca <- pca_model_best %>%
  fit(
      formula = future_return_sign ~ .,
      data = train_processed %>% select(-date)
  ) %>% 
  predict(new_data = test_processed,type='prob') %>% 
  bind_cols(test) %>% mutate(.pred_class=ifelse(.pred_0>0.5,0,1)) %>%
  mutate(.pred_class = as.factor(.pred_class))

pca_score_test <- test_prediction_pca %>% metrics_list(future_return_sign,estimate=.pred_class,.pred_0)

knitr::kable(pca_score_test)

```

However, PCA cannot capture our categorical `technical indicators` features. Multiple Correspondence Analysis (MCA) or Categorical PCA (CATPCA) could be used instead, but it would ignore the rest of our numerical features.  

## 5.2 Using recipeselectors  

The team found a package, `recipeselectors`, that is designed to enhance the `tidymodels` recipe package by adding supervised feature selection steps. 

```{r, echo = FALSE}
# devtools::install_github("stevenpawley/recipeselectors")
```

The package currently has 7 methods of feature selection listed below:  

* `step_select_infgain` which selects feature through Information Gain.  
* `step_select_mrmr` which selects features based on Maximum Relevancy Minimum Redundancy.  
* `step_select_roc` which selects Receiver Operating Curve (ROC)-based feature selection based on each predictors' relationship with the response outcome measured using a ROC curve.  
* `step_select_xtab` which provides feature selection through statistical association, typically for nominal variables.  
* `step_select_vip` which uses model-based selection using feature importance scores.  
* `step_select_boruta` which introduces a Boruta feature selection step.  
* `step_select_carscore` which provides a CAR score to select features, mainly in regression models.  

For this project, we will focus on exploring `Information Gain`, `Maximum Relevancy Minimum Redundancy` and `Boruta` feature selection steps.  

```{r}
library(recipeselectors)
```

### 5.2.1 Information Gain

```{r}
# select features first
recipe_select_infgain <- recipe_select_base %>% 
  step_select_infgain(all_predictors(), outcome = "future_return_sign", threshold = .99) 

# new dataframe
infgain_prep <- juice(prep(recipe_select_infgain))

# Train test split infgain_prep
infgain_split <- rsample::initial_time_split(infgain_prep,prop=0.8)
train <- training(infgain_split)
test <- testing(infgain_split)


rm(recipe_select_infgain)
# new recipe
recipe_final_infgain <- recipe(future_return_sign ~., data = infgain_prep) %>% 
  update_role(date, future_return, close, new_role = "ID")

# getting resamples
resamples_infgain_expanding <- recipe_final_infgain %>% 
  prep() %>% juice() %>% 
  time_series_cv(datevar = date, 
                 initial = '3 month', 
                 assess = '3 month', 
                 skip = '3 month', 
                 cumulative = T)

#choice of model - cat

infgain_wflw <- workflow() %>% add_recipe(recipe_final_infgain) %>% add_model(selected_model)

infgain_trained <- tune_grid(
  infgain_wflw,
  grid = selected_params, 
  metrics = metric_set(mn_log_loss), 
  resamples = resamples_infgain_expanding,
  control = control_resamples(verbose = F, save_pred = T, allow_par = T)
)
```

After which we will evaluate the performance 

```{r}
best_inf_params <- infgain_trained %>%
  select_best('mn_log_loss', maximise = FALSE)

inf_model_best <- selected_model %>%
  finalize_model(best_inf_params)

inf_wflw_best <- workflow() %>%
  add_recipe(recipe_final_infgain) %>%
  add_model(inf_model_best)

train_processed <- bake(recipe_final_infgain %>% prep(),  new_data = train)
train_prediction_inf <- inf_model_best %>%
  fit(
      formula = future_return_sign ~ .,
      data = train_processed %>% select(-date)
  ) %>% 
  predict(new_data = train_processed,type='prob') %>% 
  bind_cols(train) %>% mutate(.pred_class=ifelse(.pred_0>0.5,0,1)) %>%
  mutate(.pred_class = as.factor(.pred_class))

metrics_list = metric_set(mn_log_loss)

inf_score_train <- train_prediction_inf %>% 
  metrics_list(truth=future_return_sign,estimate=.pred_class,.pred_0)

knitr::kable(inf_score_train)

#Performance on Testing
test_processed <- bake(recipe_final_infgain %>% prep(),  new_data = test)

test_prediction_inf <- inf_model_best %>%
  fit(
      formula = future_return_sign ~ .,
      data = train_processed %>% select(-date)
  ) %>% 
  predict(new_data = test_processed,type='prob') %>% 
  bind_cols(test) %>% mutate(.pred_class=ifelse(.pred_0>0.5,0,1)) %>%
  mutate(.pred_class = as.factor(.pred_class))

inf_score_test <- test_prediction_inf %>% metrics_list(future_return_sign,estimate=.pred_class,.pred_0)

knitr::kable(inf_score_test)
```

### 5.2.2 Maximum Relevancy Minimum Redundancy

```{r}
#select features first
recipe_select_mrmr <- recipe_select_base %>% 
  step_select_mrmr(all_predictors(), 
                   outcome = "future_return_sign", threshold = .99)

# new dataframe
mrmr_prep <- juice(prep(recipe_select_mrmr))

# Train test split 
mrmr_split <- rsample::initial_time_split(mrmr_prep,prop=0.8)
train <- training(mrmr_split)
test <- testing(mrmr_split)

rm(recipe_select_mrmr)

# new recipe
recipe_final_mrmr <- recipe(future_return_sign ~., data = mrmr_prep) %>% 
  update_role(date, future_return, close, new_role = "ID")

# resamples
resamples_mrmr_expanding <- recipe_final_mrmr %>% 
  prep() %>% juice() %>% 
  time_series_cv(datevar = date, 
                 initial = '3 month', 
                 assess = '3 month', 
                 skip = '3 month', 
                 cumulative = T)

# choice of model - cat

mrmr_wflw <- workflow() %>% add_recipe(recipe_final_mrmr) %>% add_model(selected_model)

mrmr_trained <- tune_grid(
  mrmr_wflw,
  grid = selected_params, 
  metrics = metric_set(mn_log_loss), 
  resamples = resamples_mrmr_expanding,
  control = control_resamples(verbose = F, save_pred = T, allow_par = T)
)
```

After which we will evaluate the performance: 

```{r}
best_mrmr_params <- mrmr_trained %>%
  select_best('mn_log_loss', maximise = FALSE)

mrmr_model_best <- selected_model %>%
  finalize_model(best_mrmr_params)

mrmr_wflw_best <- workflow() %>%
  add_recipe(recipe_final_mrmr) %>%
  add_model(mrmr_model_best)

train_processed <- bake(recipe_final_mrmr %>% prep(),  new_data = train)
train_prediction_mrmr <- mrmr_model_best %>%
  fit(
      formula = future_return_sign ~ .,
      data = train_processed %>% select(-date)
  ) %>% 
  predict(new_data = train_processed,type='prob') %>% 
  bind_cols(train) %>% mutate(.pred_class=ifelse(.pred_0>0.5,0,1)) %>% 
  mutate(.pred_class = as.factor(.pred_class))

metrics_list = metric_set(mn_log_loss)

mrmr_score_train <- train_prediction_mrmr %>% 
  metrics_list(truth=future_return_sign,estimate=.pred_class,.pred_0)

knitr::kable(mrmr_score_train)

#Performance on Testing
test_processed <- bake(recipe_final_mrmr %>% prep(),  new_data = test)

test_prediction_mrmr <- mrmr_model_best %>%
  fit(
      formula = future_return_sign ~ .,
      data = train_processed %>% select(-date)
  ) %>% 
  predict(new_data = test_processed,type='prob') %>% 
  bind_cols(test) %>% mutate(.pred_class=ifelse(.pred_0>0.5,0,1)) %>%
  mutate(.pred_class = as.factor(.pred_class))

mrmr_score_test <- test_prediction_mrmr %>% metrics_list(future_return_sign,estimate=.pred_class,.pred_0)

knitr::kable(mrmr_score_test)

```

### 5.2.3 Boruta  

```{r}
#WARNING: Takes very long to run

# select features
recipe_select_boruta <- recipe_select_base %>% step_select_boruta(all_predictors(), outcome = "future_return_sign") 

# new dataframe
boruta_prep <- juice(prep(recipe_select_boruta))
rm(recipe_select_boruta)

# Train test split 
boruta_split <- rsample::initial_time_split(boruta_prep,prop=0.8)
train <- training(boruta_split)
test <- testing(boruta_split)

# new recipe
recipe_final_boruta <- recipe(future_return_sign ~., data = boruta_prep) %>% 
  update_role(date, future_return, close, new_role = "ID")

# resamples
resamples_boruta_expanding <- recipe_final_boruta %>% 
  prep() %>% juice() %>% 
  time_series_cv(datevar = date, 
                 initial = '3 month', 
                 assess = '3 month', 
                 skip = '3 month', 
                 cumulative = T)

# model of choice - cat

boruta_wflw <- workflow() %>% add_recipe(recipe_final_boruta) %>% add_model(selected_model)

boruta_trained <- tune_grid(
  boruta_wflw,
  grid = selected_params, 
  metrics = metric_set(mn_log_loss), 
  resamples = resamples_boruta_expanding,
  control = control_resamples(verbose = F, save_pred = T, allow_par = T)
)
```

After which we will evaluate the performance

```{r}
best_boruta_params <- boruta_trained %>%
  select_best('mn_log_loss', maximise = FALSE)

boruta_model_best <- selected_model %>%
  finalize_model(best_boruta_params)

boruta_wflw_best <- workflow() %>%
  add_recipe(recipe_final_boruta) %>%
  add_model(boruta_model_best)

train_processed <- bake(recipe_final_boruta %>% prep(),  new_data = train)
train_prediction_boruta <- boruta_model_best %>%
  fit(
      formula = future_return_sign ~ .,
      data = train_processed %>% select(-date)
  ) %>% 
  predict(new_data = train_processed,type='prob') %>% 
  bind_cols(train) %>% mutate(.pred_class=ifelse(.pred_0>0.5,0,1)) %>%
  mutate(.pred_class = as.factor(.pred_class))

metrics_list = metric_set(mn_log_loss)

boruta_score_train <- train_prediction_boruta %>% 
  metrics_list(truth=future_return_sign,estimate=.pred_class,.pred_0)

knitr::kable(boruta_score_train)

#Performance on Testing
test_processed <- bake(recipe_final_boruta %>% prep(),  new_data = test)

test_prediction_boruta <- boruta_model_best %>%
  fit(
      formula = future_return_sign ~ .,
      data = train_processed %>% select(-date)
  ) %>% 
  predict(new_data = test_processed,type='prob') %>% 
  bind_cols(test) %>% mutate(.pred_class=ifelse(.pred_0>0.5,0,1)) %>%
  mutate(.pred_class = as.factor(.pred_class))

boruta_score_test <- test_prediction_boruta %>% metrics_list(future_return_sign,estimate=.pred_class,.pred_0)

knitr::kable(boruta_score_test)

```

## 5.3 Feature Summaries

```{r}
feat_summaries <- rbind(pca_score_train, 
                        pca_score_test, 
                        inf_score_train, 
                        inf_score_test, 
                        mrmr_score_train, 
                        mrmr_score_test, 
                        boruta_score_train, 
                        boruta_score_test) 
feat_summaries$Method <- c(rep("PCA", 2), rep("Inf. Gain", 2), rep("MRMR", 2), rep("Boruta", 2))
feat_summaries$Data <- rep(c("Train", "Test"), 4)

feat_summaries <- feat_summaries %>% select(Method, Data, .metric, .estimate) %>% arrange(.estimate)

knitr::kable(feat_summaries)
```

## 5.4 Feature-Selected Model Performance

```{r}
stratcomp_inf <- test_prediction_inf %>% mutate(.pred_class = as.numeric(.pred_class)-1) %>% 
  mutate(`Buy & Hold` = ROC(close, n = 1) %>% lead(), 
         `Inf. Catboost` = .pred_class*`Buy & Hold`) %>%
  select(date, `Buy & Hold`, `Inf. Catboost`)

stratcomp_pca <- test_prediction_pca %>% mutate(.pred_class = as.numeric(.pred_class)-1) %>%
  left_join(stratcomp_inf %>% select(date, `Buy & Hold`), by = 'date') %>%
  mutate(`PCA Catboost` = .pred_class*`Buy & Hold`) %>%
  select(date,`Buy & Hold`,`PCA Catboost` )

stratcomp_mrmr <- test_prediction_mrmr %>% mutate(.pred_class = as.numeric(.pred_class)-1) %>% 
  mutate(`Buy & Hold` = ROC(close, n = 1) %>% lead(), 
         `MRMR Catboost` = .pred_class*`Buy & Hold`) %>%
  select(date, `Buy & Hold`, `MRMR Catboost`) 

stratcomp_feat <- test_prediction_boruta %>% mutate(.pred_class = as.numeric(.pred_class)-1) %>% 
  mutate(`Buy & Hold` = ROC(close, n =1) %>% lead(),
         `Boruta Catboost` = .pred_class*`Buy & Hold`) %>% 
  select(date, `Buy & Hold`, `Boruta Catboost`) %>% 
  left_join(stratcomp_pca, by = c("date", "Buy & Hold")) %>%
  left_join(stratcomp_mrmr, by = c("date", "Buy & Hold")) %>% 
  left_join(stratcomp_inf, by = c("date", "Buy & Hold")) %>%
  column_to_rownames(var = "date") %>% as.xts()

rm(stratcomp_inf, stratcomp_mrmr) # stratcomp_pca used to compare later in stacking (Section 6)

table.AnnualizedReturns(stratcomp_feat)

charts.PerformanceSummary(stratcomp_feat, main = "Feature-Selected Strategy Performance")
```

### 5.4.1 Feature-Selected Model Performance (with Cost)

```{r}
transaction_cost = 0.003

stratcomp_inf_cost <- test_prediction_inf %>% 
  mutate(.pred_class=as.numeric(.pred_class)-1) %>%
  mutate(
  `Buy & Hold` = ROC(close, n = 1) %>% lead(),
  `Inf. Catboost` = as.numeric(.pred_class)*`Buy & Hold`,
  trading_cost = (abs(.pred_class - lag(.pred_class, n = 1, default = 0))*transaction_cost), 
  `Inf. Catboost C` = `Inf. Catboost` - trading_cost
  ) %>% 
  select(date, `Buy & Hold`, `Inf. Catboost C`)

stratcomp_pca_cost <- test_prediction_pca %>% 
  left_join(stratcomp_inf_cost %>% select(date,`Buy & Hold`)) %>%
  mutate(
    .pred_class=as.numeric(.pred_class)-1,
    `PCA Catboost` = .pred_class*`Buy & Hold`,
    trading_cost = (abs(.pred_class - lag(.pred_class, n = 1, default = 0))*transaction_cost),
    `PCA Catboost C` = `PCA Catboost` - trading_cost
  ) %>%
  select(date, `Buy & Hold`, `PCA Catboost C`)

stratcomp_mrmr_cost <- test_prediction_mrmr %>% mutate(
  .pred_class=as.numeric(.pred_class)-1,
  `Buy & Hold` = ROC(close, n = 1) %>% lead(),
  `MRMR Catboost` = .pred_class*`Buy & Hold`,
  trading_cost = (abs(.pred_class - lag(.pred_class, n = 1, default = 0))*transaction_cost), 
  `MRMR Catboost C` = `MRMR Catboost` - trading_cost) %>% 
  select(date, `Buy & Hold`, `MRMR Catboost C`)

stratcomp_feat_cost <- test_prediction_boruta %>% mutate(
  .pred_class=as.numeric(.pred_class)-1,
  `Buy & Hold` = ROC(close, n = 1) %>% lead(),
  `Boruta Catboost` = .pred_class*`Buy & Hold`,
  trading_cost = (abs(.pred_class - lag(.pred_class, n = 1, default = 0))*transaction_cost),
  `Boruta Catboost C` = `Boruta Catboost` - trading_cost) %>% 
  select(date, `Buy & Hold`, `Boruta Catboost C`) %>% 
  left_join(stratcomp_pca_cost, by = c("date", "Buy & Hold")) %>%
  left_join(stratcomp_inf_cost, by = c("date", "Buy & Hold")) %>%
  left_join(stratcomp_mrmr_cost, by = c("date", "Buy & Hold")) %>%
  column_to_rownames(var = "date") %>% as.xts()

rm(stratcomp_pca_cost, stratcomp_inf_cost, stratcomp_mrmr_cost)

table.AnnualizedReturns(stratcomp_feat_cost)
charts.PerformanceSummary(stratcomp_feat_cost, main = "Feature-Selected Strategy (with Cost) Performance")
```

<div id = "Section6"></div>
# 6. Stacking  

Instead of using `modeltime.ensemble`, given that our prediction model is **classification**-based, we will be using the `stacks` package which is an extension of `tidymodels`. Model stacking is another ensembling method that takes the outputs of multiple models and combines them to generate a new model, to generate predictions informed **by each model**.  

```{r}
# remotes::install_github("tidymodels/stacks", ref = "main")
library(stacks)
```

## 6.1 Defining Recipe  

To successfully for an ensemble, each model must share the same resample. Since our `PCA` feature selection yielded the best results, we will use this dataset as our base across all models. 

```{r}
# new recipe
recipe_select_pca <- recipe_select_base %>% 
  step_nzv(all_predictors()) %>% 
  step_normalize(all_numeric(), - all_outcomes()) %>% 
  step_pca(all_predictors(), num_comp = 10, id = "pca")

# dataframe
pca_prep <- juice(prep(recipe_select_pca))

# Train test split pca_prep
pca_split <- rsample::initial_time_split(pca_prep,prop=0.8)
train <- training(pca_split)
test <- testing(pca_split)

rm(pca_estimates, pca_info, recipe_select_pca)

# new recipe
recipe_final_pca <- recipe(future_return_sign ~., data = train) %>% 
  update_role(date, new_role = "ID")

# getting resamples
resamples_pca_expanding <- recipe_final_pca %>% 
  prep() %>% juice() %>% 
  time_series_cv(datevar = date, 
                 initial = '3 month', 
                 assess = '3 month', 
                 skip = '3 month', 
                 cumulative = T)
```

## 6.2 Tuning Control  

Now that we have defined our base recipes and models, we have to first create a `stack_control` and we would have to save these predictions from each model as the ensemble will be using these predictions to generate the coefficients.  

```{r}
stack_control <- control_grid(save_pred = T, save_workflow = T)
stack_metrics <- metric_set(roc_auc, mn_log_loss, accuracy)
```

## 6.3 Defining Grids  

As from Section 4, we have defined our model parameters as well:  

```{r}
xgb_params
cat_params
log_params
```

## 6.4 Defining Workflows  

```{r}
xgb_wflw_stack <- workflow() %>% add_model(xgb_model) %>% 
  add_recipe(recipe_final_pca)

cat_wflw_stack <- workflow() %>% add_model(cat_model) %>% 
  add_recipe(recipe_final_pca)

log_wflw_stack <- workflow() %>% add_model(log_model) %>% 
  add_recipe(recipe_final_pca)
```

## 6.5 Tuning Model with Resamples

```{r}
xgb_res <- tune_grid(
  xgb_wflw_stack,
  resamples = resamples_pca_expanding,
  grid = xgb_params,
  metrics = stack_metrics,
  control = stack_control
)

cat_res <- tune_grid(
  cat_wflw_stack,
  resamples = resamples_pca_expanding,
  grid = cat_params,
  metrics = stack_metrics,
  control = stack_control
)

log_res <- tune_grid(
  log_wflw_stack,
  resamples = resamples_pca_expanding,
  grid = log_params,
  metrics = stack_metrics,
  control = stack_control
)

```

## 6.6 Stacking Models 

Once we have defined the above, we can start by initiating `stacks()` and then to `add_candidates()` which would be the different models we have explored. We can then `blend_predictions()` to find out what is the combination of each model to be used in our final ensemble.

```{r}
stack_model <- stacks() %>% 
  add_candidates(xgb_res) %>% 
  add_candidates(cat_res) %>% 
  add_candidates(log_res) 

stack_weights <- stack_model %>% blend_predictions()
# saveRDS(stack_weights, file = "./stack_weights.RDS")
# stack_weights <- readRDS("./stack_weights.RDS")
```

We can see the above weights that the ensemble recommends to put on each model.  

`blend_predictions()` helps to remove predictors with no influence and once we pipe this through `fit_members()`, our model stack would be trained based on our input models and can predict on new data.  

```{r}
final_stack <- stack_weights %>% fit_members()
# saveRDS(final_stack, file = "./final_stack.RDS")
# final_stack <- readRDS("./final_stack.RDS")
```

Essentially what `stacks()` has helped us to do, is to condense multiple lines of code which delves into:  
* Finding the best parameters for each model and using that best model  
* Doing a LASSO regression to find the coefficients of each model into the ensemble  

We can see how efficient `stacks()` would be if there were more and more models to combine and to predict upon.

## 6.7 Testing Stack   

Once we get our final stack model, `final_stack`, we can use it to predict on our `test` data. 

```{r}
stack_pred <- predict(final_stack, new_data = test, type = 'prob') %>% 
  bind_cols(test) %>% mutate(.pred_class = ifelse(.pred_0 > 0.5, 0, 1)) %>% 
  mutate(.pred_class = as.factor(.pred_class)) 

stack_pred_perf <- stack_pred %>%
  metrics_list(future_return_sign,estimate=.pred_class,.pred_0)

stack_pred %>% yardstick::accuracy(truth = future_return_sign, estimate = .pred_class)

knitr::kable(stack_pred_perf)
```

```{r}
stratcomp_stack <- stack_pred %>% mutate(.pred_class = as.numeric(.pred_class)-1) %>% 
  left_join(stratcomp_pca %>% select(date,`Buy & Hold`)) %>%
  mutate(`Stack` = .pred_class*`Buy & Hold`) %>%
  select(date, `Buy & Hold`, `Stack`) %>% column_to_rownames(var = "date") %>% as.xts()


table.AnnualizedReturns(stratcomp_stack)

charts.PerformanceSummary(stratcomp_stack)
```

### 6.7.1 Stack Model Performance (with Cost)

```{r}
transaction_cost = 0.003

stratcomp_stack_cost <- stack_pred %>%
  mutate(.pred_class = as.numeric(.pred_class)-1) %>% 
  left_join(stratcomp_pca %>% select(date,`Buy & Hold`)) %>%
  mutate(
    `Stack` = .pred_class*`Buy & Hold`,
    trading_cost = (abs(.pred_class - lag(.pred_class, n = 1, default = 0))*transaction_cost),
    `Stack C` = `Stack`- trading_cost
  ) %>%
  select(date, `Buy & Hold`,`Stack C`) %>%
  column_to_rownames(var = "date") %>% as.xts()

table.AnnualizedReturns(stratcomp_stack_cost)
charts.PerformanceSummary(stratcomp_stack_cost, main = "Stack Model (with Cost) Performance")
```

After stacking, we arrive at an optimised model with the highest Sharpe Ratio of more than 2.3, indicating that this model might be the best model in terms of risk adjusted returns. 

# 7. Conclusion  

**XGB** and **Catboost** models had the best performance, and these models are typically prone to overfitting. By using an array of feature selection methods, we have reduced the overall number of features significantly, lowering the probability of overfitting. `tidymodels` has made it much easier and efficient to experiment with different features. As BTC continues to change and as the world gradually accepts its value, this paradigm shift in the realm of cryptocurrency would also change the features we explored as listed in [Section 3](#Section3). `tidyverse` made it easier to simply `join` these features together.  

These features could also be easily explored and reduced using `tidymodels` native functions such as `step_pca()`. Package extensions such as `recipeselectors` explored in [Section 5](#Section5) also synergises with `tidymodels`. By using inbuilt functions, it opens up the opportunity to explore several feature reduction methods available, all building on the `recipe` set and the `workflow`.  

Lastly, `tidymodels` has also made it easier to explore these features **across** different models. With `stacks()`, we could have condensed [Section 4](#Section4) on Predictive Model Exploration and Evaluation into just [Section 6](#Section6), and have the confidence that `stacking` would pull the best out of each model and combine it into a single model.  

## 7.1 Limitations 

This report serves to highlight the effectiveness of `tidymodels` and we recognise that the models produced here are in no more the most optimal ones. 

Firstly, we did not tune all the hyperparameters of each model and only focus on tuning at most one hyperparameter. With enough computational power, we would have tune more hyperparameters and possible make the model more complex since we have taken steps to ensure that overfitting problems would be addressed subsequently. 

Secondly, the number of models used. Our team only looked at 3 models, **XGB**, **Catboost** and **Logistic Regression**, and this is by no means an indication that these models are the best and produces the best results. Other models that we have not included could also possibly be used and be lead to a better performance.  


# 8. Appendices  

<div id="AppendixA"></div>
## 8.1. Appendix A (LGBM Workflow)    

In LGBM, hyperparameters for `no. of leaves`, `min_data_in_leaf` and `max_depth` are the most important features. As of now, we were unable to obtain number of leaves, thus, we will tune for `min_data_in_leaf (min_n)` and `max_depth (tree_depth)`.  

Implementing LGBM into [Section 4](#Section4):  

```{r, eval = FALSE}
# install.packages("lightgbm")
library(lightgbm)

### DEFINING MODEL ###
set_dependency("boost_tree", eng = "lightgbm", pkg = "treesnip")
lgbm_model <- boost_tree(learn_rate = 0.01,
                          tree_depth = tune(),
                          min_n = tune(),
                          mtry = 500,
                          trees = 500,
                          stop_iter = 50) %>%
    set_engine('lightgbm') %>%
    set_mode('classification')

### DEFINING WORKFLOW ###
lgbm_wflw <- workflow() %>%
   add_recipe(recipe_spec) %>%
   add_model(lgbm_model)

### DEFINING PARAMETERS ###
lgbm_params <- grid_max_entropy(parameters(min_n(), tree_depth()), size = 10)

### SETTING UP PARALLEL PROCESSING ###
all_cores <- parallel::detectCores(logical=F)
registerDoParallel(cores = all_cores)

### TUNING PARAMETERS ###
lgbm_model_trained <- tune_grid(
   lgbm_wflw, 
   grid = lgbm_params,
   metrics = metric_set(mn_log_loss),
   resamples = resamples_cv_expanding,
   control = control_resamples(verbose = T,
                               save_pred = TRUE,
                               allow_par = TRUE))

### SELECTING BEST MODEL ###
best_params <- lgbm_model_trained %>%
   select_best('mn_log_loss', maximise = FALSE)
lgbm_model_best <- lgbm_model %>%
   finalize_model(best_params)
 
lgbm_wflw_best <- workflow() %>%
   add_recipe(recipe_spec) %>%
   add_model(lgbm_model_best)

### TRAIN MODEL PERFORMANCE ###
train_processed <- bake(recipe_spec %>% prep(),  new_data = train)
 
train_prediction_lgb <- lgbm_model_best %>%
   fit(
       formula = future_return_sign ~ .,
       data = train_processed %>% select(-date)
   ) %>% 
  predict(new_data = train_processed,type='prob') %>% 
  bind_cols(train) %>% mutate(.pred_class=ifelse(.pred_0>0.5,0,1))
 
metrics_list = metric_set(mn_log_loss)
 
lgbm_score_train <- train_prediction_lgb %>% 
   metrics_list(truth=future_return_sign,estimate=.pred_class,.pred_0)
 
knitr::kable(lgbm_score_train)
 
### TEST MODEL PERFORMANCE ###
test_processed <- bake(recipe_spec %>% prep(),  new_data = test)
 
test_prediction_lgb <- lgbm_model_best %>%
   fit(
       formula = future_return_sign ~ .,
       data = train_processed %>% select(-date)
   ) %>% 
   predict(new_data = test_processed,type='prob') %>% 
   bind_cols(test) %>% mutate(.pred_class=ifelse(.pred_0>0.5,0,1))
 
lgbm_score_test <- test_prediction_lgb %>% metrics_list(future_return_sign,estimate=.pred_class,.pred_0)
 
knitr::kable(lgbm_score_test)
```





